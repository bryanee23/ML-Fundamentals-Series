{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597264323480",
   "display_name": "Python 3.7.4 64-bit ('ML': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road Map\n",
    "\n",
    "Specific Tools\n",
    "- preprocessing.StandardScaler()\n",
    "- sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Clean \n",
    "> - verify data types of dataset\n",
    "> - remove any values from the dataset which are not needed\n",
    "> - convert datatypes to integers if necessary\n",
    "> - tools: astypes, dtypes, LabelEncoding\n",
    "\n",
    "Prepare \n",
    "> - convert dataset into an np.array\n",
    "> - normalize the data if the values vary signicantly\n",
    "> - split dataset into training and testing sets\n",
    "\n",
    "Model \n",
    "> - select features\n",
    "> - set outcome\n",
    "> - fit training sets into chosen alogrithm\n",
    "> - transform data onto unseen datasets\n",
    "> - predict outcome using testing set\n",
    "\n",
    "Evaluate\n",
    "> - compare testing and predicted outcomes\n",
    "> - use Mean Squred Error, F1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing:\n",
    "\n",
    "1. select features\n",
    "\n",
    "2. convert into values into an array\n",
    "\n",
    "3. normalize the values of the features\n",
    "> From the docs\n",
    "\n",
    "> - sklearn.preprocessing.StandardScaler(*, copy=True, with_mean=True, with_std=True)\n",
    "\n",
    "> - Standardize features by removing the mean and scaling to unit variance\n",
    "\n",
    "Methods:\n",
    "> - fit(X, y) : Compute the mean and std to be used for later scaling.\n",
    "\n",
    "> - fit_transform(X, y) : Fit to data, then transform it.\n",
    "\n",
    "> - transform(X, copy) : Perform standardization by centering and scaling\n",
    "\n",
    "4. split the data into training and testing sets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Modeling\n",
    "\n",
    "1. fit data into logistic regression Modeling\n",
    "\n",
    "> - using predict and predict_proba\n",
    "\n",
    "> - from the docs\n",
    "\n",
    "> - sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "Parameters\n",
    "\n",
    "> - C : float, default=1.0\n",
    "> Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization\n",
    "\n",
    "> - solver : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’\n",
    "\n",
    "> For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.\n",
    "\n",
    "> For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.\n",
    "\n",
    "> - ‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty\n",
    "> - ‘liblinear’ and ‘saga’ also handle L1 penalty\n",
    "> - ‘saga’ also supports ‘elasticnet’ penalty\n",
    "> - ‘liblinear’ does not support setting penalty='none'\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Log Loss: 0.602 \n\nF1 score: 0.545 \n\n              precision    recall  f1-score   support\n\n         0.0       0.73      0.96      0.83        25\n         1.0       0.86      0.40      0.55        15\n\n   micro avg       0.75      0.75      0.75        40\n   macro avg       0.79      0.68      0.69        40\nweighted avg       0.78      0.75      0.72        40\n\nconfusion matrix \n [[24  1]\n [ 9  6]]\n"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn import preprocessing, linear_model, metrics, model_selection\n",
    "\n",
    "df = pd.read_csv('datasets/ChurnData.csv')\n",
    "# clean\n",
    "df.astype(int).dtypes\n",
    "\n",
    "# prepare\n",
    "columns = ['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip', 'callcard', 'wireless', 'longmon', 'tollmon', 'equipmon', 'cardmon', 'wiremon', 'longten', 'tollten', 'cardten', 'voice', 'pager', 'internet', 'callwait', 'confer', 'ebill', 'loglong', 'logtoll','lninc', 'custcat', 'churn']\n",
    "\n",
    "features = ['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']\n",
    "\n",
    "\n",
    "output = ['churn']\n",
    "\n",
    "x = df[features].values\n",
    "y = df[output].values\n",
    "\n",
    "x = preprocessing.StandardScaler().fit(x).transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size=0.2, random_state=4)\n",
    "\n",
    "# model\n",
    "LogReg = linear_model.LogisticRegression(C=0.01, solver='liblinear').fit(x_train, y_train)\n",
    "\n",
    "y_hat = LogReg.predict(x_test)\n",
    "y_prob = LogReg.predict_proba(x_test)\n",
    "\n",
    "# evaluate\n",
    "log_loss = metrics.log_loss(y_test, y_prob)\n",
    "print('Log Loss: %.3f' % log_loss, '\\n')\n",
    "\n",
    "f1_score = metrics.f1_score(y_test, y_hat)\n",
    "print('F1 score: %.3f' % f1_score, '\\n')\n",
    "\n",
    "classifcation_report = metrics.classification_report(y_test, y_hat)\n",
    "print(classifcation_report)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_hat)\n",
    "print('confusion matrix \\n', confusion_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation\n",
    "\n",
    "- log_loss\n",
    "\n",
    "> log loss(Logarithmic loss) measures the performance of a classifier where the predicted output is a probability value between 0 and 1\n",
    "\n",
    "- f1_score\n",
    "\n",
    "> The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. \n",
    "\n",
    "- classification report\n",
    "\n",
    "- confusion matrix\n",
    "\n",
    "> Build a text report showing the main classification metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Disclaimer</h2>\n",
    "\n",
    "This script was orginally from Coursera's [IBM AI Engineering course](https://www.coursera.org/professional-certificates/ai-engineer), authored by <a href=\"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a> and was modifed to fit my needs. "
   ]
  }
 ]
}